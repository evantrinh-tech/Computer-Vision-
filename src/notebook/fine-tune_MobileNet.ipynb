{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30caa508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\logging\\__init__.py\", line 1113, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f680' in position 16: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\1120627266.py\", line 166, in <module>\n",
      "    trainer.fit(\"FineTune\", epochs=50, lr=1e-4, freeze=False) # Cháº¡y max 50 epoch nhÆ°ng Early Stopping sáº½ tá»± ngáº¯t sá»›m\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\1120627266.py\", line 102, in fit\n",
      "    self.logger.info(f\"\\nðŸš€ {stage_name}\")\n",
      "Message: '\\nðŸš€ FineTune'\n",
      "Arguments: ()\n",
      "INFO:__main__:\n",
      "ðŸš€ FineTune\n",
      "INFO:__main__:Epoch 1 | Loss: 0.6753 Acc: 0.5756 | Val Loss: 0.6451 Val Acc: 0.7442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.645075). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 2 | Loss: 0.5639 Acc: 0.8430 | Val Loss: 0.5245 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.645075 --> 0.524459). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 3 | Loss: 0.4550 Acc: 0.9360 | Val Loss: 0.4384 Val Acc: 0.9302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.524459 --> 0.438396). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 4 | Loss: 0.3549 Acc: 0.9651 | Val Loss: 0.3492 Val Acc: 0.9302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.438396 --> 0.349164). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 5 | Loss: 0.2794 Acc: 0.9709 | Val Loss: 0.2673 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.349164 --> 0.267314). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 6 | Loss: 0.2052 Acc: 0.9884 | Val Loss: 0.2140 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.267314 --> 0.213971). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 7 | Loss: 0.1335 Acc: 0.9942 | Val Loss: 0.1646 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.213971 --> 0.164585). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 8 | Loss: 0.1130 Acc: 0.9884 | Val Loss: 0.1310 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.164585 --> 0.130979). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 9 | Loss: 0.0722 Acc: 0.9942 | Val Loss: 0.1199 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.130979 --> 0.119946). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 10 | Loss: 0.0447 Acc: 0.9942 | Val Loss: 0.1134 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.119946 --> 0.113360). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 11 | Loss: 0.0402 Acc: 0.9942 | Val Loss: 0.0967 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.113360 --> 0.096664). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 12 | Loss: 0.0429 Acc: 0.9884 | Val Loss: 0.0814 Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.096664 --> 0.081362). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 13 | Loss: 0.0332 Acc: 1.0000 | Val Loss: 0.0778 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.081362 --> 0.077769). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 14 | Loss: 0.0214 Acc: 1.0000 | Val Loss: 0.0701 Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.077769 --> 0.070126). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 15 | Loss: 0.0489 Acc: 0.9826 | Val Loss: 0.0862 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 16 | Loss: 0.0353 Acc: 0.9942 | Val Loss: 0.0684 Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.070126 --> 0.068379). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 17 | Loss: 0.0256 Acc: 0.9942 | Val Loss: 0.0748 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 18 | Loss: 0.0135 Acc: 1.0000 | Val Loss: 0.0902 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 19 | Loss: 0.0173 Acc: 1.0000 | Val Loss: 0.0868 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 20 | Loss: 0.0174 Acc: 1.0000 | Val Loss: 0.0570 Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.068379 --> 0.056995). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 21 | Loss: 0.0161 Acc: 1.0000 | Val Loss: 0.0921 Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 22 | Loss: 0.0100 Acc: 1.0000 | Val Loss: 0.0909 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 23 | Loss: 0.0155 Acc: 1.0000 | Val Loss: 0.0694 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 24 | Loss: 0.0102 Acc: 1.0000 | Val Loss: 0.0904 Val Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 25 | Loss: 0.0089 Acc: 1.0000 | Val Loss: 0.0570 Val Acc: 0.9535\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\logging\\__init__.py\", line 1113, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u1eeb' in position 44: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\1120627266.py\", line 166, in <module>\n",
      "    trainer.fit(\"FineTune\", epochs=50, lr=1e-4, freeze=False) # Cháº¡y max 50 epoch nhÆ°ng Early Stopping sáº½ tá»± ngáº¯t sá»›m\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\1120627266.py\", line 147, in fit\n",
      "    self.logger.warning(\"Early Stopping triggered! Dá»«ng Ä‘á»ƒ trÃ¡nh Overfitting.\")\n",
      "Message: 'Early Stopping triggered! Dá»«ng Ä‘á»ƒ trÃ¡nh Overfitting.'\n",
      "Arguments: ()\n",
      "WARNING:__main__:Early Stopping triggered! Dá»«ng Ä‘á»ƒ trÃ¡nh Overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "DATA_DIR = r\"d:\\Computer Vision\\Computer-Vision Project\\Computer-Vision-\\data\\images\"\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. EARLY STOPPING CLASS ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# --- 2. TRAINER CLASS (VERSION 3.0: SMOOTH + EARLY STOP) ---\n",
    "class ModelTrainer:\n",
    "    def __init__(self, data_path: str, run_name: str = \"traffic_pro\"):\n",
    "        self.device = DEVICE\n",
    "        self.data_path = Path(data_path)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.run_dir = Path(f\"models/{run_name}_{timestamp}\")\n",
    "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO, handlers=[logging.FileHandler(self.run_dir/\"train.log\"), logging.StreamHandler()])\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Load Model (Sá»­ dá»¥ng kiáº¿n trÃºc class TrafficIncidentModel cÅ© cá»§a báº¡n)\n",
    "        try:\n",
    "            from torchvision.models import EfficientNet_B0_Weights\n",
    "            self.model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT).to(self.device)\n",
    "        except:\n",
    "            self.model = models.efficientnet_b0(pretrained=True).to(self.device)\n",
    "            \n",
    "        # Thay tháº¿ head cho binary classification\n",
    "        in_ftrs = self.model.classifier[1].in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_ftrs, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = datasets.ImageFolder(self.data_path)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        # Transform (Simple & Effective)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        train_set.dataset.transform = transform\n",
    "        val_set.dataset.transform = transform\n",
    "        self.train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def fit(self, stage_name: str, epochs: int, lr: float, freeze: bool = True):\n",
    "        self.logger.info(f\"\\nðŸš€ {stage_name}\")\n",
    "        # Quáº£n lÃ½ Ä‘Ã³ng bÄƒng lá»›p\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"classifier\" not in name: param.requires_grad = not freeze\n",
    "            \n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr)\n",
    "        early_stopping = EarlyStopping(patience=5, verbose=True) # Dá»«ng náº¿u sau 5 epoch khÃ´ng giáº£m loss\n",
    "        best_model_path = self.run_dir / f\"best_{stage_name.lower().replace(' ', '_')}.pth\"\n",
    "        \n",
    "        history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            t_loss, t_correct = 0, 0\n",
    "            for imgs, lbls in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                imgs, lbls = imgs.to(self.device), lbls.to(self.device).float().view(-1, 1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(imgs)\n",
    "                loss = self.criterion(outputs, lbls)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t_loss += loss.item() * imgs.size(0)\n",
    "                t_correct += torch.sum((outputs > 0.5).float() == lbls.data)\n",
    "            \n",
    "            self.model.eval()\n",
    "            v_loss, v_correct = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for imgs, lbls in self.val_loader:\n",
    "                    imgs, lbls = imgs.to(self.device), lbls.to(self.device).float().view(-1, 1)\n",
    "                    outputs = self.model(imgs)\n",
    "                    v_loss += self.criterion(outputs, lbls).item() * imgs.size(0)\n",
    "                    v_correct += torch.sum((outputs > 0.5).float() == lbls.data)\n",
    "            \n",
    "            res = {\n",
    "                'loss': t_loss / len(self.train_loader.dataset),\n",
    "                'acc': (t_correct.double() / len(self.train_loader.dataset)).item(),\n",
    "                'val_loss': v_loss / len(self.val_loader.dataset),\n",
    "                'val_acc': (v_correct.double() / len(self.val_loader.dataset)).item()\n",
    "            }\n",
    "            for k in history: history[k].append(res[k])\n",
    "            self.logger.info(f\"Epoch {epoch+1} | Loss: {res['loss']:.4f} Acc: {res['acc']:.4f} | Val Loss: {res['val_loss']:.4f} Val Acc: {res['val_acc']:.4f}\")\n",
    "\n",
    "            # --- Gá»ŒI EARLY STOPPING ---\n",
    "            early_stopping(res['val_loss'], self.model, best_model_path)\n",
    "            if early_stopping.early_stop:\n",
    "                self.logger.warning(\"Early Stopping triggered! Dá»«ng Ä‘á»ƒ trÃ¡nh Overfitting.\")\n",
    "                break\n",
    "\n",
    "        self._plot_smooth(history, stage_name)\n",
    "\n",
    "    def _plot_smooth(self, history, title):\n",
    "        def smooth(data, w=0.6):\n",
    "            res = [data[0]]\n",
    "            for p in data[1:]: res.append(res[-1]*w + p*(1-w))\n",
    "            return res\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1); plt.plot(smooth(history['acc']), label='Train'); plt.plot(smooth(history['val_acc']), label='Val'); plt.title(f\"{title} Acc\"); plt.legend()\n",
    "        plt.subplot(1, 2, 2); plt.plot(smooth(history['loss']), label='Train'); plt.plot(smooth(history['val_loss']), label='Val'); plt.title(f\"{title} Loss\"); plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# --- CHáº Y ---\n",
    "trainer = ModelTrainer(DATA_DIR)\n",
    "trainer.prepare_data()\n",
    "trainer.fit(\"FineTune\", epochs=50, lr=1e-4, freeze=False) # Cháº¡y max 50 epoch nhÆ°ng Early Stopping sáº½ tá»± ngáº¯t sá»›m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a56475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\logging\\__init__.py\", line 1113, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f680' in position 16: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Python\\pythoncore-3.11-64\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\237377630.py\", line 170, in <module>\n",
      "    trainer.fit(\"Stage 1 - Transfer Learning\", epochs=20, lr=1e-3, freeze=True)\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_26576\\237377630.py\", line 88, in fit\n",
      "    self.logger.info(f\"\\nðŸš€ {stage_name}\")\n",
      "Message: '\\nðŸš€ Stage 1 - Transfer Learning'\n",
      "Arguments: ()\n",
      "INFO:__main__:\n",
      "ðŸš€ Stage 1 - Transfer Learning\n",
      "INFO:__main__:Epoch 1 | Loss: 0.6855 Acc: 0.5465 | Val Loss: 0.6681 Val Acc: 0.7209\n",
      "                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    168\u001b[39m trainer = ModelTrainer(data_path=DATA_DIR, run_name=\u001b[33m\"\u001b[39m\u001b[33mtraffic_pro\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m trainer.prepare_data()\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStage 1 - Transfer Learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m trainer.fit(\u001b[33m\"\u001b[39m\u001b[33mStage 2 - Fine Tuning\u001b[39m\u001b[33m\"\u001b[39m, epochs=\u001b[32m30\u001b[39m, lr=\u001b[32m5e-6\u001b[39m, freeze=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# LR cá»±c nhá» Ä‘á»ƒ cá»±c mÆ°á»£t\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mModelTrainer.fit\u001b[39m\u001b[34m(self, stage_name, epochs, lr, freeze)\u001b[39m\n\u001b[32m    104\u001b[39m smooth_lbls = lbls * (\u001b[32m1\u001b[39m - \u001b[32m0.1\u001b[39m) + \u001b[32m0.05\u001b[39m\n\u001b[32m    106\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, smooth_lbls)\n\u001b[32m    109\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mTrafficIncidentModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classifier(\u001b[38;5;28mself\u001b[39m.pool(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:165\u001b[39m, in \u001b[36mMBConv.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_res_connect:\n\u001b[32m    167\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.stochastic_depth(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2813\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2811\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import datasets, models, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd # DÃ¹ng Ä‘á»ƒ lÃ m má»‹n biá»ƒu Ä‘á»“\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# from datetime import datetime\n",
    "\n",
    "# # --- Cáº¤U HÃŒNH ---\n",
    "# DATA_DIR = r\"d:\\Computer Vision\\Computer-Vision Project\\Computer-Vision-\\data\\images\"\n",
    "# IMAGE_SIZE = 224\n",
    "# BATCH_SIZE = 16\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # --- 1. MODEL DEFINITION (EfficientNet-B0) ---\n",
    "# class TrafficIncidentModel(nn.Module):\n",
    "#     def __init__(self, use_pretrained=True):\n",
    "#         super(TrafficIncidentModel, self).__init__()\n",
    "#         try:\n",
    "#             from torchvision.models import EfficientNet_B0_Weights\n",
    "#             self.backbone = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT if use_pretrained else None).features\n",
    "#         except:\n",
    "#             self.backbone = models.efficientnet_b0(pretrained=use_pretrained).features\n",
    "        \n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Dropout(0.4), # TÄƒng dropout Ä‘á»ƒ chá»‘ng giáº­t\n",
    "#             nn.Linear(1280, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.classifier(self.pool(self.backbone(x)))\n",
    "\n",
    "# # --- 2. TRAINER CLASS ---\n",
    "# class ModelTrainer:\n",
    "#     def __init__(self, data_path: str, run_name: str = \"train\"):\n",
    "#         self.device = DEVICE\n",
    "#         self.data_path = Path(data_path)\n",
    "#         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         self.run_dir = Path(f\"models/{run_name}_{timestamp}\")\n",
    "#         self.run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         logging.basicConfig(level=logging.INFO, handlers=[logging.FileHandler(self.run_dir/\"train.log\"), logging.StreamHandler()])\n",
    "#         self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "#         self.model = TrafficIncidentModel().to(self.device)\n",
    "#         # Sá»¬ Dá»¤NG LABEL SMOOTHING (BÃ­ kÃ­p Ä‘á»ƒ lÃ m má»‹n Loss)\n",
    "#         self.criterion = nn.BCELoss() \n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         # Giáº£m bá»›t cÆ°á»ng Ä‘á»™ Augmentation Ä‘á»ƒ á»•n Ä‘á»‹nh hÆ¡n\n",
    "#         train_transform = transforms.Compose([\n",
    "#             transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomRotation(15),\n",
    "#             transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "#         val_transform = transforms.Compose([\n",
    "#             transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "        \n",
    "#         dataset = datasets.ImageFolder(self.data_path)\n",
    "#         train_size = int(0.8 * len(dataset))\n",
    "#         val_size = len(dataset) - train_size\n",
    "#         train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "#         train_set.dataset.transform = train_transform\n",
    "#         val_set.dataset.transform = val_transform\n",
    "#         self.train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#         self.val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "#     def fit(self, stage_name: str, epochs: int, lr: float, freeze: bool = True):\n",
    "#         self.logger.info(f\"\\nðŸš€ {stage_name}\")\n",
    "#         for param in self.model.backbone.parameters(): param.requires_grad = not freeze\n",
    "        \n",
    "#         # ThÃªm Weight Decay Ä‘á»ƒ lÃ m mÆ°á»£t trá»ng sá»‘\n",
    "#         optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=lr, weight_decay=1e-4)\n",
    "#         scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(self.train_loader), epochs=epochs)\n",
    "        \n",
    "#         history = {'loss': [], 'val_loss': [], 'acc': [], 'val_acc': []}\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             self.model.train()\n",
    "#             t_loss, t_correct = 0, 0\n",
    "#             for imgs, lbls in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "#                 imgs, lbls = imgs.to(self.device), lbls.to(self.device).float().view(-1, 1)\n",
    "                \n",
    "#                 # Label Smoothing thá»§ cÃ´ng (0 -> 0.05, 1 -> 0.95)\n",
    "#                 smooth_lbls = lbls * (1 - 0.1) + 0.05\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = self.model(imgs)\n",
    "#                 loss = self.criterion(outputs, smooth_lbls)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "                \n",
    "#                 t_loss += loss.item() * imgs.size(0)\n",
    "#                 t_correct += torch.sum((outputs > 0.5).float() == lbls.data)\n",
    "            \n",
    "#             self.model.eval()\n",
    "#             v_loss, v_correct = 0, 0\n",
    "#             with torch.no_grad():\n",
    "#                 for imgs, lbls in self.val_loader:\n",
    "#                     imgs, lbls = imgs.to(self.device), lbls.to(self.device).float().view(-1, 1)\n",
    "#                     outputs = self.model(imgs)\n",
    "#                     v_loss += self.criterion(outputs, lbls).item() * imgs.size(0)\n",
    "#                     v_correct += torch.sum((outputs > 0.5).float() == lbls.data)\n",
    "            \n",
    "#             res = {\n",
    "#                 'loss': t_loss / len(self.train_loader.dataset),\n",
    "#                 'acc': (t_correct.double() / len(self.train_loader.dataset)).item(),\n",
    "#                 'val_loss': v_loss / len(self.val_loader.dataset),\n",
    "#                 'val_acc': (v_correct.double() / len(self.val_loader.dataset)).item()\n",
    "#             }\n",
    "#             for k in history: history[k].append(res[k])\n",
    "#             self.logger.info(f\"Epoch {epoch+1} | Loss: {res['loss']:.4f} Acc: {res['acc']:.4f} | Val Loss: {res['val_loss']:.4f} Val Acc: {res['val_acc']:.4f}\")\n",
    "\n",
    "#         self._plot_smooth_results(history, stage_name)\n",
    "\n",
    "#     def _plot_smooth_results(self, history, title):\n",
    "#         def smooth_data(data, weight=0.6): # HÃ m lÃ m má»‹n Exponential Moving Average\n",
    "#             last = data[0]\n",
    "#             smoothed = []\n",
    "#             for point in data:\n",
    "#                 low_pass = last * weight + (1 - weight) * point\n",
    "#                 smoothed.append(low_pass)\n",
    "#                 last = low_pass\n",
    "#             return smoothed\n",
    "\n",
    "#         plt.figure(figsize=(14, 5))\n",
    "        \n",
    "#         # Váº½ Accuracy\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.plot(history['acc'], alpha=0.2, color='blue')\n",
    "#         plt.plot(smooth_data(history['acc']), label='Train Acc (Smoothed)', color='blue', linewidth=2)\n",
    "#         plt.plot(history['val_acc'], alpha=0.2, color='orange')\n",
    "#         plt.plot(smooth_data(history['val_acc']), label='Val Acc (Smoothed)', color='orange', linewidth=2)\n",
    "#         plt.title(f\"{title} - Accuracy (Smooth)\"); plt.legend()\n",
    "\n",
    "#         # Váº½ Loss\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.plot(history['loss'], alpha=0.2, color='blue')\n",
    "#         plt.plot(smooth_data(history['loss']), label='Train Loss (Smoothed)', color='blue', linewidth=2)\n",
    "#         plt.plot(history['val_loss'], alpha=0.2, color='orange')\n",
    "#         plt.plot(smooth_data(history['val_loss']), label='Val Loss (Smoothed)', color='orange', linewidth=2)\n",
    "#         plt.title(f\"{title} - Loss (Smooth)\"); plt.legend()\n",
    "\n",
    "#         plt.savefig(self.run_dir / f\"smooth_chart_{title.lower().replace(' ', '_')}.png\")\n",
    "#         plt.show()\n",
    "\n",
    "# # --- CHáº Y ---\n",
    "# trainer = ModelTrainer(data_path=DATA_DIR, run_name=\"traffic_pro\")\n",
    "# trainer.prepare_data()\n",
    "# trainer.fit(\"Stage 1 - Transfer Learning\", epochs=20, lr=1e-3, freeze=True)\n",
    "# trainer.fit(\"Stage 2 - Fine Tuning\", epochs=30, lr=5e-6, freeze=False) # LR cá»±c nhá» Ä‘á»ƒ cá»±c mÆ°á»£t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627fc9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Computer Vision\\Computer-Vision Project\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CHá»ˆNH ÄÆ¯á»œNG DáºªN Äáº¾N FILE MODEL Cá»¦A Báº N Táº I ÄÃ‚Y ---\n",
    "MODEL_PATH = r\"models/traffic_pro_20260119_200303/best_finetune.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Äá»ŠNH NGHÄ¨A Láº I KIáº¾N TRÃšC (Báº¯t buá»™c pháº£i giá»‘ng lÃºc train)\n",
    "def load_trained_model(model_path):\n",
    "    # Load xÆ°Æ¡ng sá»‘ng EfficientNet-B0\n",
    "    model = models.efficientnet_b0(pretrained=False) # KhÃ´ng cáº§n táº£i láº¡i weights ImageNet\n",
    "    \n",
    "    # Thay Head giá»‘ng y há»‡t lÃºc train\n",
    "    in_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(in_ftrs, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    # Äá»• trá»ng sá»‘ Ä‘Ã£ luyá»‡n vÃ o\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 2. CHUáº¨N Bá»Š INPUT (Pháº£i giá»‘ng lÃºc train)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 3. HÃ€M Dá»° ÄOÃN áº¢NH Báº¤T Ká»²\n",
    "def predict_image(image_path, model):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prob = model(img_tensor).item()\n",
    "    \n",
    "    # NgÆ°á»¡ng 0.5: <0.5 lÃ  Incident (theo alphabet I < N), >0.5 lÃ  Normal\n",
    "    # LÆ°u Ã½: Báº¡n hÃ£y kiá»ƒm tra láº¡i trainer.train_loader.dataset.dataset.classes \n",
    "    # Náº¿u Incident lÃ  0, Normal lÃ  1:\n",
    "    label = \"INCIDENT âš ï¸\" if prob < 0.5 else \"NORMAL âœ…\"\n",
    "    confidence = (1 - prob) if prob < 0.5 else prob\n",
    "    \n",
    "    # Hiá»ƒn thá»‹\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Dá»± Ä‘oÃ¡n: {label}\\nÄá»™ tin cáº­y: {confidence*100:.2f}%\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return label, prob\n",
    "\n",
    "# --- CHáº Y THá»¬ NGHIá»†M ---\n",
    "# Báº¡n hÃ£y Ä‘Æ°a Ä‘Æ°á»ng dáº«n má»™t táº¥m áº£nh báº¥t ká»³ vÃ o Ä‘Ã¢y\n",
    "model = load_trained_model(MODEL_PATH)\n",
    "# VÃ­ dá»¥: test_img = r\"d:\\Computer Vision\\Computer-Vision Project\\Computer-Vision-\\data\\images\\normal\\some_img.jpg\"\n",
    "# predict_image(test_img, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
